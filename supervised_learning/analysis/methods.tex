\section{Description of Methods}
Five algorithms were run: AdaBoost (with decision tree estimator), \textit{k}-nearest neighbors, decision tree (with Gini index), neural network, and support vector machines. Using the Python library \href{http://scikit-learn.org/stable/}{\texttt{scikit-learn}}, the data sets were split into 70-30 training-test, each algorithm was run with 10x cross validation methods on the training set, and a grid search algorithm was applied to determine the `best' hyperparameters for each algorithm from the test. From this, the best model was then chosen to be trained and tested over the data. A learning curve was generated from this run. \textit{Note: due to lack of support, the neural networks were run with} \href{http://pybrain.org}{\texttt{PyBrain}} \textit{without any grid search optimization.}